"""
The Refiner Agent - TDFlow Test Generation

Implements Test-Driven Flow (TDFlow) from "Building Code Testing Agents" PDF.

Key Concept: The Refiner generates the test suite BEFORE content is created,
ensuring coverage optimization and edge case discovery.

Goal Vector: "Coverage Optimization"
Key Metric: Edge Case Discovery
Failure Mode: Triviality (generating obvious/useless tests)
"""

from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class TestType(Enum):
    """Types of tests the Refiner can generate"""
    UNIT = "unit"
    INTEGRATION = "integration"
    SEMANTIC = "semantic"
    ADVERSARIAL = "adversarial"
    BOUNDARY = "boundary"
    REGRESSION = "regression"


@dataclass
class GeneratedTest:
    """A test generated by the Refiner agent"""
    name: str
    description: str
    test_type: TestType
    target_component: str
    input_specification: Dict[str, Any]
    expected_behavior: str
    edge_cases: List[str] = field(default_factory=list)
    coverage_score: float = 0.0
    triviality_score: float = 0.0  # Higher = more trivial (bad)
    
    def is_non_trivial(self) -> bool:
        """Check if test is worth running (not trivial)"""
        return self.triviality_score < 0.3


@dataclass
class TDFlowPlan:
    """Test-Driven Flow plan generated before content creation"""
    component_name: str
    tests: List[GeneratedTest]
    coverage_target: float
    estimated_coverage: float
    created_before_content: bool = True


class RefinerAgent:
    """
    The Refiner Agent - Implements TDFlow methodology.
    
    From PDF:
    "Enforce a policy where the 'Refiner' agent generates the test suite
    BEFORE the 'Instructional Design' agent generates the content."
    
    This ensures:
    1. Tests define expected behavior upfront
    2. Edge cases are identified before implementation
    3. Coverage gaps are addressed proactively
    4. Triviality is avoided (no obvious/useless tests)
    """
    
    def __init__(self, llm_client, config: Optional[Dict] = None):
        """
        Initialize Refiner Agent.
        
        Args:
            llm_client: LLM for test generation
            config: Optional configuration for coverage targets
        """
        self.llm = llm_client
        self.config = config or {}
        self.generated_plans: List[TDFlowPlan] = []
        
        # Default thresholds
        self.coverage_target = self.config.get('coverage_target', 0.80)
        self.triviality_threshold = self.config.get('triviality_threshold', 0.3)
    
    async def generate_test_plan(
        self,
        component_spec: Dict[str, Any],
        existing_tests: Optional[List[str]] = None
    ) -> TDFlowPlan:
        """
        Generate test plan BEFORE content/implementation.
        
        This is the core TDFlow concept - tests first, implementation second.
        
        Args:
            component_spec: Specification of component to be built
            existing_tests: Optional list of existing tests to avoid duplication
        
        Returns:
            TDFlowPlan with generated tests
        """
        logger.info(f"Generating TDFlow plan for: {component_spec.get('name', 'unknown')}")
        
        prompt = self._build_test_generation_prompt(component_spec, existing_tests)
        
        try:
            response = await self.llm.generate(prompt)
            tests = self._parse_test_response(response, component_spec)
            
            # Filter trivial tests
            non_trivial_tests = [t for t in tests if t.is_non_trivial()]
            
            # Calculate coverage
            estimated_coverage = self._estimate_coverage(non_trivial_tests, component_spec)
            
            plan = TDFlowPlan(
                component_name=component_spec.get('name', 'unknown'),
                tests=non_trivial_tests,
                coverage_target=self.coverage_target,
                estimated_coverage=estimated_coverage,
                created_before_content=True
            )
            
            self.generated_plans.append(plan)
            
            logger.info(
                f"Generated {len(non_trivial_tests)} non-trivial tests "
                f"(coverage: {estimated_coverage:.0%})"
            )
            
            return plan
            
        except Exception as e:
            logger.error(f"Test generation failed: {e}")
            return TDFlowPlan(
                component_name=component_spec.get('name', 'unknown'),
                tests=[],
                coverage_target=self.coverage_target,
                estimated_coverage=0.0
            )
    
    def _build_test_generation_prompt(
        self,
        component_spec: Dict[str, Any],
        existing_tests: Optional[List[str]]
    ) -> str:
        """Build prompt for test generation"""
        
        existing_str = "\n".join(existing_tests) if existing_tests else "None"
        
        return f"""You are The Refiner Agent implementing TDFlow (Test-Driven Flow).

GOAL VECTOR: Coverage Optimization
KEY METRIC: Edge Case Discovery  
FAILURE MODE TO AVOID: Triviality (obvious/useless tests)

COMPONENT SPECIFICATION:
{component_spec}

EXISTING TESTS (avoid duplication):
{existing_str}

TASK: Generate a comprehensive test suite BEFORE implementation.

For each test, provide:
1. Name and description
2. Test type (unit, integration, semantic, adversarial, boundary, regression)
3. Input specification
4. Expected behavior
5. Edge cases to cover
6. Coverage score (0.0-1.0) - how much of the component this tests
7. Triviality score (0.0-1.0) - higher means more obvious/useless

REQUIREMENTS:
- Focus on edge cases and boundary conditions
- Include semantic tests that check meaning, not just syntax
- Include adversarial scenarios
- Avoid trivial tests (e.g., "test that function exists")
- Each test must add unique coverage value

Respond in JSON array format:
[
  {{
    "name": "test_name",
    "description": "what this tests",
    "test_type": "semantic",
    "target_component": "component.function",
    "input_specification": {{"key": "value"}},
    "expected_behavior": "description",
    "edge_cases": ["case1", "case2"],
    "coverage_score": 0.15,
    "triviality_score": 0.1
  }}
]
"""
    
    def _parse_test_response(
        self,
        response: str,
        component_spec: Dict[str, Any]
    ) -> List[GeneratedTest]:
        """Parse LLM response into GeneratedTest objects"""
        import json
        
        try:
            # Try direct JSON parse
            data = json.loads(response)
        except json.JSONDecodeError:
            # Extract from code block
            if '```json' in response:
                start = response.find('```json') + 7
                end = response.find('```', start)
                data = json.loads(response[start:end].strip())
            elif '```' in response:
                start = response.find('```') + 3
                end = response.find('```', start)
                data = json.loads(response[start:end].strip())
            else:
                return []
        
        tests = []
        for item in data:
            test = GeneratedTest(
                name=item.get('name', 'unnamed_test'),
                description=item.get('description', ''),
                test_type=TestType(item.get('test_type', 'unit')),
                target_component=item.get('target_component', component_spec.get('name', '')),
                input_specification=item.get('input_specification', {}),
                expected_behavior=item.get('expected_behavior', ''),
                edge_cases=item.get('edge_cases', []),
                coverage_score=float(item.get('coverage_score', 0.0)),
                triviality_score=float(item.get('triviality_score', 0.5))
            )
            tests.append(test)
        
        return tests
    
    def _estimate_coverage(
        self,
        tests: List[GeneratedTest],
        component_spec: Dict[str, Any]
    ) -> float:
        """Estimate coverage from test list (simplified)"""
        if not tests:
            return 0.0
        
        # Sum coverage scores, cap at 1.0
        total = sum(t.coverage_score for t in tests)
        return min(total, 1.0)
    
    def get_coverage_gaps(self, plan: TDFlowPlan) -> List[str]:
        """Identify coverage gaps in a test plan"""
        gaps = []
        
        if plan.estimated_coverage < plan.coverage_target:
            gaps.append(
                f"Coverage gap: {plan.estimated_coverage:.0%} < {plan.coverage_target:.0%} target"
            )
        
        # Check for missing test types
        test_types = {t.test_type for t in plan.tests}
        recommended_types = {TestType.UNIT, TestType.SEMANTIC, TestType.BOUNDARY}
        missing = recommended_types - test_types
        
        for missing_type in missing:
            gaps.append(f"Missing {missing_type.value} tests")
        
        return gaps
